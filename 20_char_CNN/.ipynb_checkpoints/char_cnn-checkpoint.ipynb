{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference\n",
    "\n",
    "nlp/nlp-character_embedding.ipynb at master Â· makcedward/nlp  \n",
    "\n",
    "https://github.com/makcedward/nlp/blob/master/sample/nlp-character_embedding.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hanl9/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# making data frame from csv file \n",
    "df = pd.read_csv('abstract.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>FOR</th>\n",
       "      <th>field_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Histograms of oriented gradients for human det...</td>\n",
       "      <td>We study the question of feature sets for robu...</td>\n",
       "      <td>801</td>\n",
       "      <td>Artificial Intelligence and Image Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep residual learning for image recognition</td>\n",
       "      <td>Deeper neural networks are more difficult to t...</td>\n",
       "      <td>801</td>\n",
       "      <td>Artificial Intelligence and Image Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond bags of features: Spatial pyramid match...</td>\n",
       "      <td>This paper presents a method for recognizing s...</td>\n",
       "      <td>801</td>\n",
       "      <td>Artificial Intelligence and Image Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Going deeper with convolutions</td>\n",
       "      <td>We propose a deep convolutional neural network...</td>\n",
       "      <td>801</td>\n",
       "      <td>Artificial Intelligence and Image Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rich feature hierarchies for accurate object d...</td>\n",
       "      <td>Object detection performance, as measured on t...</td>\n",
       "      <td>801</td>\n",
       "      <td>Artificial Intelligence and Image Processing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Histograms of oriented gradients for human det...   \n",
       "1       Deep residual learning for image recognition   \n",
       "2  Beyond bags of features: Spatial pyramid match...   \n",
       "3                     Going deeper with convolutions   \n",
       "4  Rich feature hierarchies for accurate object d...   \n",
       "\n",
       "                                            Abstract  FOR  \\\n",
       "0  We study the question of feature sets for robu...  801   \n",
       "1  Deeper neural networks are more difficult to t...  801   \n",
       "2  This paper presents a method for recognizing s...  801   \n",
       "3  We propose a deep convolutional neural network...  801   \n",
       "4  Object detection performance, as measured on t...  801   \n",
       "\n",
       "                                       field_name  \n",
       "0   Artificial Intelligence and Image Processing   \n",
       "1   Artificial Intelligence and Image Processing   \n",
       "2   Artificial Intelligence and Image Processing   \n",
       "3   Artificial Intelligence and Image Processing   \n",
       "4   Artificial Intelligence and Image Processing   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/hanl9/anaconda3/lib/python3.7/site-packages (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: h5py in /home/hanl9/anaconda3/lib/python3.7/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /home/hanl9/anaconda3/lib/python3.7/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied: tensorflow in /home/hanl9/anaconda3/lib/python3.7/site-packages (1.13.0rc1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.13.0rc0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.18.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: h5py in /home/hanl9/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /home/hanl9/anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n",
      "Requirement already satisfied: pbr>=0.11 in /home/hanl9/anaconda3/lib/python3.7/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow) (5.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install keras\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Modeling\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, GlobalMaxPool1D, Bidirectional\n",
    "from keras.layers import LSTM, Lambda, Bidirectional, concatenate, BatchNormalization, Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import IPython\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= df[['FOR','Abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns=['category','headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>801</td>\n",
       "      <td>We study the question of feature sets for robu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>801</td>\n",
       "      <td>Deeper neural networks are more difficult to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>801</td>\n",
       "      <td>This paper presents a method for recognizing s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>801</td>\n",
       "      <td>We propose a deep convolutional neural network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>801</td>\n",
       "      <td>Object detection performance, as measured on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           headline\n",
       "0       801  We study the question of feature sets for robu...\n",
       "1       801  Deeper neural networks are more difficult to t...\n",
       "2       801  This paper presents a method for recognizing s...\n",
       "3       801  We propose a deep convolutional neural network...\n",
       "4       801  Object detection performance, as measured on t..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN:\n",
    "    __author__ = \"Edward Ma\"\n",
    "    __copyright__ = \"Copyright 2018, Edward Ma\"\n",
    "    __credits__ = [\"Edward Ma\"]\n",
    "    __license__ = \"Apache\"\n",
    "    __version__ = \"2.0\"\n",
    "    __maintainer__ = \"Edward Ma\"\n",
    "    __email__ = \"makcedward@gmail.com\"\n",
    "    \n",
    "    CHAR_DICT = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .!?:,\\'%-\\(\\)/$|&;[]\"'\n",
    "    \n",
    "    def __init__(self, max_len_of_sentence, max_num_of_setnence, verbose=10):\n",
    "        self.max_len_of_sentence = max_len_of_sentence\n",
    "        self.max_num_of_setnence = max_num_of_setnence\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.num_of_char = 0\n",
    "        self.num_of_label = 0\n",
    "        self.unknown_label = ''\n",
    "        \n",
    "    def build_char_dictionary(self, char_dict=None, unknown_label='UNK'):\n",
    "        \"\"\"\n",
    "            Define possbile char set. Using \"UNK\" if character does not exist in this set\n",
    "        \"\"\" \n",
    "        \n",
    "        if char_dict is None:\n",
    "            char_dict = self.CHAR_DICT\n",
    "            \n",
    "        self.unknown_label = unknown_label\n",
    "\n",
    "        chars = []\n",
    "\n",
    "        for c in char_dict:\n",
    "            chars.append(c)\n",
    "\n",
    "        chars = list(set(chars))\n",
    "        \n",
    "        chars.insert(0, unknown_label)\n",
    "\n",
    "        self.num_of_char = len(chars)\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "        \n",
    "        if self.verbose > 5:\n",
    "            print('Totoal number of chars:', self.num_of_char)\n",
    "\n",
    "            print('First 3 char_indices sample:', {k: self.char_indices[k] for k in list(self.char_indices)[:3]})\n",
    "            print('First 3 indices_char sample:', {k: self.indices_char[k] for k in list(self.indices_char)[:3]})\n",
    "            \n",
    "\n",
    "        return self.char_indices, self.indices_char, self.num_of_char\n",
    "    \n",
    "    def convert_labels(self, labels):\n",
    "        \"\"\"\n",
    "            Convert label to numeric\n",
    "        \"\"\"\n",
    "        self.label2indexes = dict((l, i) for i, l in enumerate(labels))\n",
    "        self.index2labels = dict((i, l) for i, l in enumerate(labels))\n",
    "\n",
    "        if self.verbose > 5:\n",
    "            print('Label to Index: ', self.label2indexes)\n",
    "            print('Index to Label: ', self.index2labels)\n",
    "            \n",
    "        self.num_of_label = len(self.label2indexes)\n",
    "\n",
    "        return self.label2indexes, self.index2labels\n",
    "    \n",
    "    def _transform_raw_data(self, df, x_col, y_col, label2indexes=None, sample_size=None):\n",
    "        \"\"\"\n",
    "            ##### Transform raw data to list\n",
    "        \"\"\"\n",
    "        \n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        actual_max_sentence = 0\n",
    "        \n",
    "        if sample_size is None:\n",
    "            sample_size = len(df)\n",
    "\n",
    "        for i, row in df.head(sample_size).iterrows():\n",
    "            x_data = row[x_col]\n",
    "            y_data = row[y_col]\n",
    "\n",
    "            sentences = sent_tokenize(x_data)\n",
    "            x.append(sentences)\n",
    "\n",
    "            if len(sentences) > actual_max_sentence:\n",
    "                actual_max_sentence = len(sentences)\n",
    "\n",
    "            y.append(label2indexes[y_data])\n",
    "\n",
    "        if self.verbose > 5:\n",
    "            print('Number of news: %d' % (len(x)))\n",
    "            print('Actual max sentence: %d' % actual_max_sentence)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def _transform_training_data(self, x_raw, y_raw, max_len_of_sentence=None, max_num_of_setnence=None):\n",
    "        \"\"\"\n",
    "            ##### Transform preorcessed data to numpy\n",
    "        \"\"\"\n",
    "        unknown_value = self.char_indices[self.unknown_label]\n",
    "        \n",
    "        x = np.ones((len(x_raw), max_num_of_setnence, max_len_of_sentence), dtype=np.int64) * unknown_value\n",
    "        y = np.array(y_raw)\n",
    "        \n",
    "        if max_len_of_sentence is None:\n",
    "            max_len_of_sentence = self.max_len_of_sentence\n",
    "        if max_num_of_setnence is None:\n",
    "            max_num_of_setnence = self.max_num_of_setnence\n",
    "\n",
    "        for i, doc in enumerate(x_raw):\n",
    "            for j, sentence in enumerate(doc):\n",
    "                if j < max_num_of_setnence:\n",
    "                    for t, char in enumerate(sentence[-max_len_of_sentence:]):\n",
    "                        if char not in self.char_indices:\n",
    "                            x[i, j, (max_len_of_sentence-1-t)] = self.char_indices['UNK']\n",
    "                        else:\n",
    "                            x[i, j, (max_len_of_sentence-1-t)] = self.char_indices[char]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def _build_character_block(self, block, dropout=0.3, filters=[64, 100], kernel_size=[3, 3], \n",
    "                         pool_size=[2, 2], padding='valid', activation='relu', \n",
    "                         kernel_initializer='glorot_normal'):\n",
    "        \n",
    "        for i in range(len(filters)):\n",
    "            block = Conv1D(\n",
    "                filters=filters[i], kernel_size=kernel_size[i],\n",
    "                padding=padding, activation=activation, kernel_initializer=kernel_initializer)(block)\n",
    "\n",
    "        block = Dropout(dropout)(block)\n",
    "        block = MaxPooling1D(pool_size=pool_size[i])(block)\n",
    "\n",
    "        block = GlobalMaxPool1D()(block)\n",
    "        block = Dense(128, activation='relu')(block)\n",
    "        return block\n",
    "    \n",
    "    def _build_sentence_block(self, max_len_of_sentence, max_num_of_setnence, \n",
    "                              char_dimension=16,\n",
    "                              filters=[[3, 5, 7], [200, 300, 300], [300, 400, 400]], \n",
    "#                               filters=[[100, 200, 200], [200, 300, 300], [300, 400, 400]], \n",
    "                              kernel_sizes=[[4, 3, 3], [5, 3, 3], [6, 3, 3]], \n",
    "                              pool_sizes=[[2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
    "                              dropout=0.4):\n",
    "        \n",
    "        sent_input = Input(shape=(max_len_of_sentence, ), dtype='int64')\n",
    "        embedded = Embedding(self.num_of_char, char_dimension, input_length=max_len_of_sentence)(sent_input)\n",
    "        \n",
    "        blocks = []\n",
    "        for i, filter_layers in enumerate(filters):\n",
    "            blocks.append(\n",
    "                self._build_character_block(\n",
    "                    block=embedded, filters=filters[i], kernel_size=kernel_sizes[i], pool_size=pool_sizes[i])\n",
    "            )\n",
    "\n",
    "        sent_output = concatenate(blocks, axis=-1)\n",
    "        sent_output = Dropout(dropout)(sent_output)\n",
    "        sent_encoder = Model(inputs=sent_input, outputs=sent_output)\n",
    "\n",
    "        return sent_encoder\n",
    "    \n",
    "    def _build_document_block(self, sent_encoder, max_len_of_sentence, max_num_of_setnence, \n",
    "                             num_of_label, dropout=0.3, \n",
    "                             loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']):\n",
    "        doc_input = Input(shape=(max_num_of_setnence, max_len_of_sentence), dtype='int64')\n",
    "        doc_output = TimeDistributed(sent_encoder)(doc_input)\n",
    "\n",
    "        doc_output = Bidirectional(LSTM(128, return_sequences=False, dropout=dropout))(doc_output)\n",
    "\n",
    "        doc_output = Dropout(dropout)(doc_output)\n",
    "        doc_output = Dense(128, activation='relu')(doc_output)\n",
    "        doc_output = Dropout(dropout)(doc_output)\n",
    "        doc_output = Dense(num_of_label, activation='sigmoid')(doc_output)\n",
    "\n",
    "        doc_encoder = Model(inputs=doc_input, outputs=doc_output)\n",
    "        doc_encoder.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        return doc_encoder\n",
    "    \n",
    "    def preporcess(self, labels, char_dict=None, unknown_label='UNK'):\n",
    "        if self.verbose > 3:\n",
    "            print('-----> Stage: preprocess')\n",
    "            \n",
    "        self.build_char_dictionary(char_dict, unknown_label)\n",
    "        self.convert_labels(labels)\n",
    "    \n",
    "    def process(self, df, x_col, y_col, \n",
    "                max_len_of_sentence=None, max_num_of_setnence=None, label2indexes=None, sample_size=None):\n",
    "        if self.verbose > 3:\n",
    "            print('-----> Stage: process')\n",
    "            \n",
    "        if sample_size is None:\n",
    "            sample_size = 1000\n",
    "        if label2indexes is None:\n",
    "            if self.label2indexes is None:\n",
    "                raise Exception('Does not initalize label2indexes. Please invoke preprocess step first')\n",
    "            label2indexes = self.label2indexes\n",
    "        if max_len_of_sentence is None:\n",
    "            max_len_of_sentence = self.max_len_of_sentence\n",
    "        if max_num_of_setnence is None:\n",
    "            max_num_of_setnence = self.max_num_of_setnence\n",
    "\n",
    "        x_preprocess, y_preprocess = self._transform_raw_data(\n",
    "            df=df, x_col=x_col, y_col=y_col, label2indexes=label2indexes)\n",
    "        \n",
    "        x_preprocess, y_preprocess = self._transform_training_data(\n",
    "            x_raw=x_preprocess, y_raw=y_preprocess,\n",
    "            max_len_of_sentence=max_len_of_sentence, max_num_of_setnence=max_num_of_setnence)\n",
    "        \n",
    "        if self.verbose > 5:\n",
    "            print('Shape: ', x_preprocess.shape, y_preprocess.shape)\n",
    "\n",
    "        return x_preprocess, y_preprocess\n",
    "    \n",
    "    def build_model(self, char_dimension=16, display_summary=False, display_architecture=False, \n",
    "                    loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']):\n",
    "        if self.verbose > 3:\n",
    "            print('-----> Stage: build model')\n",
    "            \n",
    "        sent_encoder = self._build_sentence_block(\n",
    "            char_dimension=char_dimension,\n",
    "            max_len_of_sentence=self.max_len_of_sentence, max_num_of_setnence=self.max_num_of_setnence)\n",
    "                \n",
    "        doc_encoder = self._build_document_block(\n",
    "            sent_encoder=sent_encoder, num_of_label=self.num_of_label,\n",
    "            max_len_of_sentence=self.max_len_of_sentence, max_num_of_setnence=self.max_num_of_setnence, \n",
    "            loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        \n",
    "        if display_architecture:\n",
    "            print('Sentence Architecture')\n",
    "            IPython.display.display(SVG(model_to_dot(sent_encoder).create(prog='dot', format='svg')))\n",
    "            print()\n",
    "            print('Document Architecture')\n",
    "            IPython.display.display(SVG(model_to_dot(doc_encoder).create(prog='dot', format='svg')))\n",
    "        \n",
    "        if display_summary:\n",
    "            print(doc_encoder.summary())\n",
    "            \n",
    "        \n",
    "        self.model = {\n",
    "            'sent_encoder': sent_encoder,\n",
    "            'doc_encoder': doc_encoder\n",
    "        }\n",
    "        \n",
    "        return doc_encoder\n",
    "    \n",
    "    def train(self, x_train, y_train, x_test, y_test, batch_size=128, epochs=1, shuffle=True):\n",
    "        if self.verbose > 3:\n",
    "            print('-----> Stage: train model')\n",
    "            \n",
    "        self.get_model().fit(\n",
    "            x_train, y_train, validation_data=(x_test, y_test), \n",
    "            batch_size=batch_size, epochs=epochs, shuffle=shuffle)\n",
    "        \n",
    "#         return self.model['doc_encoder']\n",
    "\n",
    "    def predict(self, x, return_prob=False):\n",
    "        if self.verbose > 3:\n",
    "            print('-----> Stage: predict')\n",
    "            \n",
    "        if return_prob:\n",
    "            return self.get_model().predict(x_test)\n",
    "        \n",
    "        return self.get_model().predict(x_test).argmax(axis=-1)\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model['doc_encoder']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Stage: preprocess\n",
      "Totoal number of chars: 83\n",
      "First 3 char_indices sample: {'UNK': 0, 'p': 1, 'M': 2}\n",
      "First 3 indices_char sample: {0: 'UNK', 1: 'p', 2: 'M'}\n",
      "Label to Index:  {801: 0, 803: 1, 804: 2, 805: 3, 806: 4, 807: 5, 802: 6}\n",
      "Index to Label:  {0: 801, 1: 803, 2: 804, 3: 805, 4: 806, 5: 807, 6: 802}\n",
      "-----> Stage: process\n",
      "Number of news: 21976\n",
      "Actual max sentence: 48\n",
      "Shape:  (21976, 5, 256) (21976,)\n",
      "-----> Stage: process\n",
      "Number of news: 5494\n",
      "Actual max sentence: 44\n",
      "Shape:  (5494, 5, 256) (5494,)\n",
      "-----> Stage: build model\n",
      "WARNING:tensorflow:From /home/hanl9/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/hanl9/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "-----> Stage: train model\n",
      "WARNING:tensorflow:From /home/hanl9/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/hanl9/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 21976 samples, validate on 5494 samples\n",
      "Epoch 1/3\n",
      " 2240/21976 [==>...........................] - ETA: 32:56 - loss: 1.9484 - acc: 0.1491"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Maximum number of characters per sentence is 256.\n",
    "    Maximum number of sentence is 5\n",
    "\"\"\"\n",
    "\n",
    "char_cnn = CharCNN(max_len_of_sentence=256, max_num_of_setnence=5)\n",
    "\n",
    "\"\"\"\n",
    "    First of all, we need to prepare meta information including character dictionary \n",
    "    and converting label from text to numeric (as keras support numeric input only).\n",
    "\"\"\"\n",
    "char_cnn.preporcess(labels=df['category'].unique())\n",
    "\n",
    "\"\"\"\n",
    "    We have to transform raw input training data and testing to numpy format for keras input\n",
    "\"\"\"\n",
    "x_train, y_train = char_cnn.process(\n",
    "    df=train_df, x_col='headline', y_col='category')\n",
    "x_test, y_test = char_cnn.process(\n",
    "    df=test_df, x_col='headline', y_col='category')\n",
    "\n",
    "char_cnn.build_model()\n",
    "char_cnn.train(x_train, y_train, x_test, y_test, batch_size=64, epochs=3)\n",
    "\n",
    "char_cnn.get_model().save('./char_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model_loaded = load_model('./char_cnn_model.h5')\n",
    "char_cnn_model_loaded.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
